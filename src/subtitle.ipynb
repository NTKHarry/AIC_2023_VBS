{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_2 = \"/Users/vothanhnhan/Desktop/gift/AIC2/model/data2.pkl\"\n",
    "path_data_3 = \"/Users/vothanhnhan/Desktop/gift/AIC2/model/data3.pkl\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "import os\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import faiss\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from translates1 import *\n",
    "from sequence_retrieval import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('vinai/phobert-base')\n",
    "model_sen = BertModel.from_pretrained('vinai/phobert-base').to(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pickle.load(open(path_data_2,'rb'))\n",
    "data3 = pickle.load(open(path_data_3,'rb'))\n",
    "d = 768\n",
    "print(data2[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(d)\n",
    "print(data2_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(data2[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "index = faiss.IndexFlatL2(d)\n",
    "def add_noise_to_vector(vector, noise_level=0.001):\n",
    "    noise = np.random.normal(0, noise_level, vector.shape)\n",
    "    return vector + noise\n",
    "\n",
    "noisy_vector = add_noise_to_vector(data2)\n",
    "noisy_vector /= np.linalg.norm(noisy_vector,axis=1, keepdims=True)\n",
    "index.add(noisy_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_documents(documents):\n",
    "    inputs = tokenizer(documents, return_tensors=\"pt\", padding=True, truncation=True).to(\"cpu\")\n",
    "    # Rút trích đặc trưng\n",
    "    with torch.no_grad():\n",
    "        outputs = model_sen(**inputs)\n",
    "        feature = outputs.last_hidden_state.mean(dim=1)  # Lấy vector trung bình làm đặc trưng\n",
    "    return feature.cpu().numpy()\n",
    "text_search = \"con chó\"\n",
    "find_embeddings = encode_documents(text_search).reshape((1,-1))\n",
    "find_embeddings /= np.linalg.norm(find_embeddings,axis=1, keepdims=True)\n",
    "#print(find_embeddings)\n",
    "k = 20\n",
    "D, I = index.search(find_embeddings, k)\n",
    "print(I)\n",
    "print(D)\n",
    "k = [(data3[i][0], data3[i][2], data3[i][3]) for i in I[0]]\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[[  23   65   66  278  303  360  490  491  500  524  525  754  760  840\n",
    "#   858  891  908 1071 1072 1154]]\n",
    "\n",
    "'''\n",
    "[[ 7  0  5 18  6 14  4  3  2  1  9  8 19 22 11 13 17 16 21 15]]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "with open('/Users/vothanhnhan/Desktop/gift/AIC2/src/data.pkl', 'rb') as file:  # Mở file ở chế độ ghi nhị phân (wb)\n",
    "    src = pickle.load(file) \n",
    "print(src[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đoạn text mà bạn muốn rút trích đặc trưng\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "# Tải tokenizer và mô hình TinyBERT\n",
    "tokenizer = BertTokenizer.from_pretrained('vinai/phobert-base')\n",
    "model = BertModel.from_pretrained('vinai/phobert-base').to(device)\n",
    "\n",
    "texts = [src[i][3] for i in range(len(src)-1)]\n",
    "print(texts[1])\n",
    "# Khởi tạo danh sách để lưu trữ đặc trưng\n",
    "text_features = []\n",
    "\n",
    "# Khởi tạo tqdm để theo dõi tiến trình\n",
    "for text in tqdm(texts, desc=\"Rút trích đặc trưng của text\"):\n",
    "    # Tokenize đoạn text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    # Rút trích đặc trưng\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        feature = outputs.last_hidden_state.mean(dim=1)  # Lấy vector trung bình làm đặc trưng\n",
    "        text_features.append(feature)\n",
    "\n",
    "# Chuyển list thành tensor\n",
    "#text_features = torch.cat(text_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "def extract_text_from_ttml(file_path):\n",
    "    # Parse the TTML file\n",
    "    try:\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Namespace dictionary to handle XML namespaces\n",
    "        namespaces = {'tt': 'http://www.w3.org/ns/ttml'}\n",
    "        # Extract text from <p> elements\n",
    "        texts = []\n",
    "        for elem in root.findall('.//tt:p', namespaces):\n",
    "            texts.append((file_path.split('/')[-1].split('.')[0],elem.attrib.get('begin'), elem.attrib.get('end'), elem.text))\n",
    "        return texts\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Error parsing the TTML file: {e}\")\n",
    "        return []\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "# Path to your TTML file\n",
    "file_path = '/Users/vothanhnhan/Desktop/gift/AIC2/datasets/subtitle'\n",
    "# Extract text\n",
    "extracted_texts = []\n",
    "\n",
    "i = 0\n",
    "for path in os.listdir(file_path):\n",
    "    #print(os.path.join(file_path,path))\n",
    "    extracted_texts += extract_text_from_ttml(os.path.join(file_path,path))\n",
    "\n",
    "\n",
    "# Print extracted texts\n",
    "\n",
    "cnt = 0\n",
    "for text in extracted_texts:\n",
    "    cnt += len(text[3])\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extracted_texts[:5])\n",
    "src = extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(extracted_texts,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Tạo mô hình Sentence Transformer\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "model.to(\"cpu\")\n",
    "\n",
    "# Chuyển đổi văn bản thành các vector đặc trưng\n",
    "\n",
    "\n",
    "# Danh sách các văn bản\n",
    "\n",
    "\n",
    "# Chuyển đổi văn bản thành các vector đặc trưng\n",
    "doc_embeddings = []\n",
    "for i in tqdm(range(len(src))):\n",
    "    embeddings = model.encode(src[i][3], convert_to_tensor=True, device=\"cpu\")\n",
    "    doc_embeddings.append((src[i][0], src[i][1], src[i][2], src[i][3], embeddings.cpu().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/Users/vothanhnhan/Desktop/gift/AIC2/model/data2_1.pkl\",\"rb\") as f:\n",
    "    src = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pháp một con cá hai con mèo một con cầu\n",
      "một con mèo và một con chim mưng hai xác\n",
      "con mèo trên thuyền chú mèo II và người\n",
      "để đặt cho những con mèo sắp bị triệt\n",
      "được đặt cho con mèo hoang đã bị sắp\n",
      "một con mèo cái có thể đẻ tới hai ba lứa\n",
      "600 con mèo hoang mèo sau khi Triệt sản\n",
      "chó ho con mèo đó vẫn có thể truyền\n",
      "con mèo trên thuyền như\n",
      "chó và một con mèo tham gia cuộc thi sự\n",
      "nhân J biden hóa trang thành một con mèo\n",
      "cây mới thu hút sự chú ý của các con mèo\n",
      "bản vẻ một con mèo đang ngủ rồi Anh cùng\n",
      "con mèo đang ngủ và quắp con cá Bước\n",
      "hiện trong sự kiện còn có Mouse con mèo\n",
      "những con mèo may mắn như thế này nó rất\n"
     ]
    }
   ],
   "source": [
    "text_find = \"con mèo\"\n",
    "for src_tmp in src:\n",
    "    if text_find in src_tmp[3]:\n",
    "        print(src_tmp[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(src[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "index = faiss.IndexFlatL2(768)\n",
    "x = np.array([tmp[4] for tmp in src]).reshape(-1,768)\n",
    "x /= np.linalg.norm(x,axis=1, keepdims=True)\n",
    "index.add(np.array(x).reshape(-1,768))\n",
    "def encode_documents(documents):\n",
    "    inputs = tokenizer(documents, return_tensors=\"pt\", padding=True, truncation=True).to(\"cpu\")\n",
    "    # Rút trích đặc trưng\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        feature = outputs.last_hidden_state.mean(dim=1)  # Lấy vector trung bình làm đặc trưng\n",
    "    return feature.cpu().numpy()\n",
    "def encode(documents):\n",
    "    embeddings = model.encode(documents, convert_to_tensor=True, device=\"cpu\")\n",
    "    return  embeddings.cpu().numpy()\n",
    "\n",
    "text_search = \"7:00\"\n",
    "find_embeddings = encode_documents(text_search).reshape((1,-1))\n",
    "find_embeddings /= np.linalg.norm(find_embeddings,axis=1, keepdims=True)\n",
    "#print(find_embeddings)\n",
    "k = 20\n",
    "D, I = index.search(find_embeddings, k)\n",
    "print(I)\n",
    "print(D)\n",
    "tm = [doc_embeddings[i][3] for i in I[0]]\n",
    "print(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def similarity(vector_1, vector_2):\n",
    "    \"\"\"\n",
    "    Tính toán độ tương đồng cosin giữa hai vector.\n",
    "    \n",
    "    Parameters:\n",
    "    - vector_1: np.array, vector đầu tiên.\n",
    "    - vector_2: np.array, vector thứ hai.\n",
    "    \n",
    "    Returns:\n",
    "    - similarity_score: float, điểm số độ tương đồng giữa hai vector.\n",
    "    \"\"\"\n",
    "    # Đảm bảo các vector là 2D để sử dụng cosine_similarity\n",
    "    vector_1 = np.atleast_2d(vector_1)\n",
    "    vector_2 = np.atleast_2d(vector_2)\n",
    "    \n",
    "    # Tính toán độ tương đồng cosin\n",
    "    similarity_score = cosine_similarity(vector_1, vector_2)\n",
    "    \n",
    "    return similarity_score[0][0]\n",
    "\n",
    "# Ví dụ sử dụng\n",
    "vector_a = np.array([1, 2, 3])\n",
    "x = np.array([tmp[4] for tmp in doc_embeddings]).reshape(-1,384)\n",
    "x /= np.linalg.norm(x,axis=1, keepdims=True)\n",
    "text_search = \"hoa kỳ\"\n",
    "find_embeddings = encode(text_search).reshape((1,-1))\n",
    "\n",
    "similarity_score = [(i, similarity(find_embeddings,x[i])) for i in range(x.shape[0])]\n",
    "similarity_score.sort(key = lambda x: x[1],reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(similarity_score[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([doc_embeddings[similarity_score[tmp][0]][3] for tmp in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
